{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oxk273fWvwK"
   },
   "source": [
    "# ```superautodiff``` Documentation\n",
    "#### CS207 Fall '19 Final Project\n",
    "#### Group 1: _Team Gillet_\n",
    "#### Lucie Gillet, Jussi Sakari Jukarainen, Jovin Leong, Huahua Zheng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9-e0WQ0Xn1B"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I855YlHkXpGz"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "<br>\n",
    "\n",
    "Derivatives play a critical role in the natural and applied sciences, with optimization being one of the core applications involving derivatives. Traditionally, derivatives have been approached either symbolically or through numerical analysis (_e.g._ finite differences). Although numerical approaches to solving derivatives are simple to compute, they are prone to stability issues and round-off errors. Meanwhile, although symbolic derivatives enable the evaluation of derivatives to machine precision, the process is limited by its computational intensity. Recently, the size and complexity of the functions involving derivatives have grown; these demands necessitate an alternative to symbolic and numerical methods that is able to compute derivatives with higher accuracy at a lower cost. Automatic Differentiation (AD) addresses these issues by executing a sequence of elementary arithmetic operations to compute accurate derivatives. \n",
    "\n",
    "<br>\n",
    "\n",
    "Our team aims to develop a Python package, ```superautodiff```, that implements both forward-mode and reverse-mode AD (the latter approach being our project extension). This document will review some of the the mathematical foundations behind our approach and provide relevant information on documentation and usage of ```superautodiff```. Finally, the documentation will discuss some of the underlying implementation details along with plans for how the project might develop in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tnOdVJ8XpNy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqgLFkC1XpJ8"
   },
   "source": [
    "# Background\n",
    "<br>\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "AD relies heavily on the chain rule and several other key mathematical concepts in order to compute derivatives. We now consider some background mathematical foundations that form the theoretical basis of our approach to AD.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Differential calculus**\n",
    "\n",
    "Differential calculus is concerned with the evaluation and study of gradients and/or rates of change. Numerically, we can formally define the derivative of a function $f$ evaluated at $a$ as:\n",
    "\n",
    "$$f'(a)=\\lim _{h\\to 0}{\\frac {f(a+h)-f(a)}{h}}$$.\n",
    "\n",
    "\n",
    "**Elementary functions and their derivatives**\n",
    "\n",
    "  Here are some examples of elementary functions used by AD and their corresponding derivatives:\n",
    "\n",
    "  <br>\n",
    "\n",
    "  **<center> Table 1. Elementary functions and their derivatives  </center>**\n",
    "  <br>\n",
    "\n",
    "| $$f(x)$$     | $$f'(x)$$    | \n",
    "| ------------- |:-------------:| \n",
    "| $$c$$        |         $0$   | \n",
    "| $$x$$        |         $1$   | \n",
    "| $$x^n$$      | $$nx^{n-1}$$ | \n",
    "| $$\\frac{1}{x}$$ | $$-\\frac{1}{x^2}$$ |\n",
    "| $$e^x$$      | $$e^x$$ | \n",
    "| $$log_ax$$      | $$\\frac{1}{x \\ln a}$$ | \n",
    "| $$\\ln x$$      | $$\\frac{1}{x}$$ | \n",
    "| $$\\sin(x)$$      | $$\\cos(x)$$ | \n",
    "| $$\\cos(x)$$      | $$-\\sin(x)$$ | \n",
    "| $$\\tan(x)$$      | $$\\frac{1}{\\cos^2x}$$ |<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Chain rule for composite functions**\n",
    "\n",
    "  The chain rule is a formula used to compute composite derivatives containing multiple variables. For instance, if we have a variable $z$ depending on $y$, which itself depends on $x$, we can subsequently employ the chain rule to express the derivative of $z$ with respect to $x$ is given by:\n",
    "\n",
    "<br>\n",
    "\n",
    "$${\\frac  {dz}{dx}}={\\frac  {dz}{dy}}\\cdot {\\frac  {dy}{dx}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**<center> The chain rule </center>**\n",
    "\n",
    "<br>\n",
    "\n",
    "**Forward and reverse mode**\n",
    "\n",
    "  For functions where we have intermediate components in our derivatives, we can keep track of the derivatives of each component using either of the following two modes: the forward mode and the reverse mode.\n",
    "  -        The forward mode starts with the input and computes the derivative with respect to the input using the chain rule at each subcomponent. The process involves storing the intermediate values of the derivatives of variables with respect to the input in order to evaluate the overall derivative: <br> <br> \n",
    "  $$\\frac{dw_i}{dx} = \\frac{dw_i}{dw_{i-1}}\\frac{dw_{i-1}}{dx}$$<br>\n",
    "   **<center> Forward mode </center>**  \n",
    "   \n",
    "<br>\n",
    "  \n",
    "  -        The reverse mode, meanwhile, involves both a forward pass that evaluates the values of the functions along with a backward pass that stores the derivatives of the output with respect to the different variables: <br> <br> $$\\frac{dy}{dw_i} = \\frac{dy}{dw_{i+1}}\\frac{dw_{i+1}}{dw_i}$$ <br>    **<center> Reverse mode </center>**\n",
    "\n",
    "<br>\n",
    "\n",
    "**Computational graph representation**\n",
    "\n",
    "  The elementary operations involved in the forward accumulation involved in the forward mode can be visually represented through a computational graph. For instance, the computational graph of the function $f(x)=x−\\exp\\{−2\\sin^2(4x)\\}^{[1]}$ is illustrated on Figure 1; Figure 2 presents a more complex computational graph.\n",
    "\n",
    "<br>\n",
    "\n",
    "  The graph breaks down the given function into a sequence of elementary operations that are visually charted out through the computational graph. The graph operates similarly to a flowchart and illustrates how each elementary operation modifies our initial parameter inputs in order to recover the function.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"fig/graph_1.png\" style=\"height:300px;\">\n",
    "\n",
    "**<center> Figure 1. A computational graph for $f(x)=x−\\exp\\{−2\\sin^2(4x)\\}^{[1]}$</center>**\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"fig/graph_2.png\" style=\"height:450px;\">\n",
    "\n",
    "**<center> Figure 2. A more complex computational graph</center>**\n",
    "\n",
    "<br>\n",
    "\n",
    "[1] D. Sondak, lecture 10, CS207 Fall '19\n",
    "\n",
    "<br>\n",
    "\n",
    "## What our package is doing\n",
    "Essentially, our package utilizes the aforementioned mathematical concepts in order to implement the AD through the forward mode. A primary function in our package, ```autodiff()```, takes in mathematical functions and corresponding points at which to evaluate the mathemetical functions and obtains an evaluative trace (similar to that of the graph structure above). Subsequently, this trace is used to perform differentiation on said mathematical function, using the chain rule to evaluate both the derivatives, the derivative values, and the current values at each component of the trace.\n",
    "\n",
    "Under the hood, we might perceive of the function's calculations as equivalent to populating the table illustrated in Table 2. This is basically the core of forward-mode AD; the functionality and operation of our package is discussed in greater detail in the subsequent section.\n",
    "\n",
    "<br>\n",
    "\n",
    "**<center>Table 2. An evaluation table for a foward-mode neural network</center>**\n",
    "\n",
    "| Trace | Elementary Function | Current Value | Elementary Function Derivative | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  | \n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------: | :-----------------: | \n",
    "| $x_{1}$ | $x$ | $x$ | $\\dot{ x}_{1}$ | $1$ | $0$ |\n",
    "| $x_{2}$ | $y$ | $y$ | $\\dot{x}_{2}$ | $0$ | $1$ |\n",
    "| $x_{3}$ | $w_{21}x_1$ | $w_{21}x$ | $w_{21}\\dot{x}_{1}$ | $w_{21}$ | $0$ |\n",
    "| $x_{4}$ | $w_{12}x_2$ | $w_{12}y$ | $w_{12}\\dot{x}_{2}$ | $0$ | $w_{12}$ |\n",
    "| $x_{5}$ | $w_{11}x_1$ | $w_{11}x$ | $w_{11}\\dot{x}_{1}$ | $w_{11}$ | $0$ |\n",
    "| $x_{6}$ | $w_{22}x_2$ | $w_{22}y$ | $w_{22}\\dot{x}_{2}$ | $0$ | $w_{22}$ |\n",
    "| $x_{7}$ | $x_4 + x_5$ | $w_{11}x + w_{12}y$ | $$\\dot{x}_{4} + \\dot{x}_{5}$$ | $w_{11}$ | $w_{12}$ |\n",
    "| $x_{8}$ | $x_3 + x_6$ | $w_{21}x + w_{22}y$ | $$\\dot{x}_{3} + \\dot{x}_{6}$$ | $w_{21}$ | $w_{22}$ |\n",
    "| $x_{9}$ | $z(x_7)$ | $z(w_{11}x + w_{12}y)$ | $$\\dot{x}_{7}z'(x_7)$$ | $w_{11}z'(w_{11}x + w_{12}y)$ | $w_{12}z'(w_{11}x + w_{12}y)$ |\n",
    "| $x_{10}$ | $z(x_8)$ | $z(w_{21}x + w_{22}y)$ | $$\\dot{x}_{8}z'(x_8)$$ | $w_{21}z'(w_{21}x + w_{22}y)$ | $w_{22}z'(w_{21}x + w_{22}y)$ |\n",
    "| $x_{11}$ | $w_{out,1}x_9$ | $$w_{out,1}z(w_{11}x + w_{12}y) $$  | $$w_{out,1}\\dot{x}_9$$ | $w_{out,1}w_{11}z'(w_{11}x + w_{12}y)$ | $w_{out,1}w_{12}z'(w_{11}x + w_{12}y)$ |\n",
    "| $x_{12}$ | $w_{out,2}x_{10}$ | $$w_{out,2}z(w_{21}x + w_{22}y) $$ | $$w_{out,2}\\dot{x}_{10}$$ | $w_{out,2}w_{21}z'(w_{21}x + w_{22}y)$ | $w_{out,2}w_{22}z'(w_{21}x + w_{22}y)$ |\n",
    "| $x_{13}$ | $x_{11} + x_{12}$ | $$w_{out,1}z(w_{11}x + w_{12}y) + w_{out,2}z(w_{21}x + w_{22}y) $$ | $$\\dot{x}_{11} + \\dot{x}_{12}$$ | $$w_{out,1}w_{11}z'(w_{11}x + w_{12}y) + w_{out,2}w_{21}z'(w_{21}x + w_{22}y)$$ | $$w_{out,1}w_{12}z'(w_{11}x + w_{12}y) + w_{out,2}w_{22}z'(w_{21}x + w_{22}y)$$ |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_r1OdOYlfrGh"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCWRWPWCrHHM"
   },
   "source": [
    "# How to use ```superautodiff```\n",
    "## User interaction with the package\n",
    "### Installation\n",
    "Our package will be distributed throughy PyPI (which will be detailed in the subsequent section). Users will first install the package by running (this will work only if the user has our ```requirements.txt``` file in their working directory):\n",
    "\n",
    "```pip install superautodiff -r requirements.txt```\n",
    "\n",
    "For more users who think they have the required Python dependencies and do not wish to reinstall said dependencies, the following command should be run instead:\n",
    "\n",
    "```pip install superautodiff```\n",
    "\n",
    "an alternative command which would similarly install our package is as follows:\n",
    "\n",
    "```pip install -i https://test.pypi.org/simple/ superautodiff==1.0.5```\n",
    "\n",
    "Users will then need to import ```superautodiff``` as in the above use case and will need to import our modules in order to access the package functionalities. Most importantly, users will have to import ```autodiff``` to instantiate AD objects. Subsequently, users will simply have to instantiate their functions and points within the objects in order to perform AD. For the use of the other modules, users will need to import them from our package.\n",
    "\n",
    "Alternatively if users experience some issues with the ```pip``` installation or would prefer to manually install the package, users can download or clone the repository onto their local machine. Subsequently, users need only ensure that the modules are in their working directory and should be able to import the various modules into their Python environment. \n",
    "\n",
    "The approach of downloading from our package repository is less convenient and is not recommended for basic users; however, the approach should be considered an important alternative for developers and those experiencing issues with ```pip install```.\n",
    "\n",
    "### Virtual Environments\n",
    "Although this is not required, users can choose to create a virtual environment in which the user can install ```superautodiff``` and interact with the package's functionalities within the virtual environment. We will walk through how virtual environments can be created using ```conda``` and ```virtualenv```.\n",
    "\n",
    "##### On ```virtualenv```\n",
    "Users should run the following command to set-up ```virtualenv```:\n",
    "\n",
    "```sudo easy_install virtualenv```\n",
    "\n",
    "Subsequently, users can create a virtual environment ```env``` through the following command:\n",
    "\n",
    "```virtualenv env```\n",
    "\n",
    "Next, users will need to activate the virtual environment with the following command:\n",
    "\n",
    "```source env/bin/activate```\n",
    "\n",
    "Now that the user has created and activated a virtual environment ```env```, the user should be able to either manually install ```superautodiff``` through a download or through a ```pip install```.\n",
    "\n",
    "Finally, once the user is done working in the virtual environment, the user can deactivate the virtual environment by running the following command in the terminal in the package directory:\n",
    "\n",
    "```deactivate ```\n",
    "\n",
    "##### On ```conda```\n",
    "To set up a ```conda``` environment ```env_name``` with Python at version 3.7, users should run the following command:\n",
    "\n",
    "```conda create --name env_name python=3.7```\n",
    "\n",
    "Next, users will need to activate the virtual environment with the following command:\n",
    "\n",
    "```conda activate env_name``` \n",
    "\n",
    "Alternatively, if the shell is not configured to use ```conda activate```, users can either run\n",
    "```conda init``` before activating the virtual environment or run the following command:\n",
    "\n",
    "```source activate env_name```\n",
    "\n",
    "As before, now that the user has created and activated a virtual environment ```env```, the user should be able to either manually install ```superautodiff``` through a download or through a ```pip install```.\n",
    "\n",
    "Finally, once the user is done working in the virtual environment, the user can deactivate the virtual environment by running the following command in the terminal in the package directory:\n",
    "\n",
    "```conda deactivate ```\n",
    "\n",
    "### Importing\n",
    "After installing the package, users need to subsequently import the various modules into their Python environment. For simplicity's sake, users can just import ```superautodiff``` using the following import command to retrieve all the modules in the package:\n",
    "\n",
    "```python\n",
    "import superautodiff\n",
    "```\n",
    "\n",
    "Alternatively, it is recommended that users run the following import alias command for concision and consistency: \n",
    "```python\n",
    "import superautodiff as sad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6AempE-gyjo"
   },
   "source": [
    "## Instantiating AD objects\n",
    "\n",
    "```superautodiff``` is a Python package and its core module is ```autodiff```. Within ```autodiff``` is the ```AutoDiff``` class, where class objects accepts an input $x \\in \\mathbb{R}$ (stored as the ```val``` attribute) and initializes the derivative (```der``` attribute) at $1$. The ```AutoDiff``` object then supports basic arithmetic operations (_e.g._ addition, multiplication) with integers, floats, and other ```AutoDiff``` objects. These operations will be implemented commutatively through dunder methods as appropriate. With an ```AutoDiff``` object, the user can evaluate the derivatives of a vector of functions at a specified vector of points.\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "We illustrate several use cases of our package's core functionality and show how it can be used to evaluate derivatives for functions about a given point. \n",
    "\n",
    "Summarily, the approach that is illustrated below involves importing the module and instantiating an ```AutoDiff``` object. Subsequently, users should use mathematical operations as they see fit in order to map the ```AutoDiff``` object to their target mathematical function. The usage of vectorized ```AutoDiffVector``` objects is similar.\n",
    "\n",
    "### Scalar case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jovin\\OneDrive\\Desktop\\CS207\\cs207-FinalProject\n",
      "Value of f1: 3.0;\n",
      "Value of first derivative of f1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Command to import autodiff module\n",
    "%cd ../../cs207-FinalProject\n",
    "import superautodiff as sad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Initalize variable inputs and instantiate AutoDiff object\n",
    "f1 = sad.AutoDiff(\"x\", 3.0)\n",
    "\n",
    "# Examine initial values\n",
    "print(\"Value of f1: {};\\nValue of first derivative of f1: {}\".format(f1.val, f1.der['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f1_a: 18.0;\n",
      "Value of first derivative of f1_a: 8.0\n"
     ]
    }
   ],
   "source": [
    "# For target function f_a(x) = x**2 + 2x + 3\n",
    "# We expect the value to be 18 and the value of the derivative to be 8\n",
    "f1_a = f1 ** 2 + 2 * f1 + 3\n",
    "print(\"Value of f1_a: {};\\nValue of first derivative of f1_a: {}\".format(f1_a.val, f1_a.der['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f1_b: 18.0;\n",
      "Value of first derivative of f1_b: 4.999999999999999\n"
     ]
    }
   ],
   "source": [
    "# For target function f_b(x) = cos(πx) + 5x + 4\n",
    "# We expect the value to be 18 and the value of the derivative to be 5 (approximately)\n",
    "f1_b = sad.cos(f1 * math.pi) + 5 * f1 + 4\n",
    "print(\"Value of f1_b: {};\\nValue of first derivative of f1_b: {}\".format(f1_b.val, f1_b.der['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f1_c: 8069.28115215272;\n",
      "Value of first derivative of f1_c: 24297.918449392822\n"
     ]
    }
   ],
   "source": [
    "# For target function f_c(x) = exp(3x) + 2ln(x) - 12x\n",
    "# We expect the value to be 8069.2 and the value of the derivative to be 24297.9 (approximately)\n",
    "f1_c = sad.exp(f1 * 3) + 2 * sad.log(f1) - 12 * f1\n",
    "print(\"Value of f1_c: {};\\nValue of first derivative of f1_c: {}\".format(f1_c.val, f1_c.der['x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector case\n",
    "\n",
    "We have two approaches to generating our ```AutoDiffVector``` objects that we will illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoDiffVector object:  <superautodiff.autodiff.AutoDiffVector object at 0x0000026390D74EB8>\n",
      "Value of fv_a: 3.5;\n",
      "Value of first derivative of fv_a: Counter({'a': 1.0})\n",
      "Value of fv_b: -5.0;\n",
      "Value of first derivative of fv_b: Counter({'b': 1.0})\n",
      "Value of fv_c: 7.0;\n",
      "Value of first derivative of fv_c: Counter({'c': 1.0})\n"
     ]
    }
   ],
   "source": [
    "# First appraoch using AutoDiff objects\n",
    "# Initalize variable inputs and instantiate AutoDiff objects\n",
    "fv_a = sad.AutoDiff(\"a\", 3.5)\n",
    "fv_b = sad.AutoDiff(\"b\", -5)\n",
    "fv_c = sad.AutoDiff(\"c\", 7)\n",
    "\n",
    "# Create an array of AutoDiff objects\n",
    "f_vect = [fv_a, fv_b, fv_c]\n",
    "\n",
    "# Use the array to instantiate AutoDiffVector Objects\n",
    "fv_1 = sad.AutoDiffVector(f_vect)\n",
    "\n",
    "# Examine initial values\n",
    "print(\"AutoDiffVector object: \", fv_1)\n",
    "print(\"Value of fv_a: {};\\nValue of first derivative of fv_a: {}\".format(fv_1.objects['a'].val, fv_1.objects['a'].der))\n",
    "print(\"Value of fv_b: {};\\nValue of first derivative of fv_b: {}\".format(fv_1.objects['b'].val, fv_1.objects['b'].der))\n",
    "print(\"Value of fv_c: {};\\nValue of first derivative of fv_c: {}\".format(fv_1.objects['c'].val, fv_1.objects['c'].der))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoDiffVector object:  <superautodiff.autodiff.AutoDiffVector object at 0x0000026390D77198>\n",
      "Value of fv_d: 4.0;\n",
      "Value of first derivative of fv_d: Counter({'d': 1.0})\n",
      "Value of fv_e: -1.0;\n",
      "Value of first derivative of fv_e: Counter({'e': 1.0})\n",
      "Value of fv_f: 1.1;\n",
      "Value of first derivative of fv_f: Counter({'f': 1.0})\n"
     ]
    }
   ],
   "source": [
    "# Second approach using arrays of variable names and values\n",
    "# Initialize variable and value arrays\n",
    "variables = ['d', 'e', 'f']\n",
    "values = [4,-1, 1.1]\n",
    "\n",
    "# Use vectorize to generate ADV objects\n",
    "fv_2 = sad.vectorize(variables, values)\n",
    "\n",
    "# Examine initial values\n",
    "print(\"AutoDiffVector object: \", fv_2)\n",
    "print(\"Value of fv_d: {};\\nValue of first derivative of fv_d: {}\".format(fv_2.objects['d'].val, fv_2.objects['d'].der))\n",
    "print(\"Value of fv_e: {};\\nValue of first derivative of fv_e: {}\".format(fv_2.objects['e'].val, fv_2.objects['e'].der))\n",
    "print(\"Value of fv_f: {};\\nValue of first derivative of fv_f: {}\".format(fv_2.objects['f'].val, fv_2.objects['f'].der))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fv_3 Case:\n",
      "Value of fv_a: 6.5;\n",
      "Value of first derivative of fv_a: Counter({'a': 1.0})\n",
      "Value of fv_b: -2.0;\n",
      "Value of first derivative of fv_b: Counter({'b': 1.0})\n",
      "Value of fv_c: 10.0;\n",
      "Value of first derivative of fv_c: Counter({'c': 1.0})\n",
      "fv_4 Case:\n",
      "Value of fv_a: 1.4983425606343788;\n",
      "Value of first derivative of fv_a: Counter({'a': 4.561257607252096})\n",
      "Value of fv_b: 13.522060024986343;\n",
      "Value of first derivative of fv_b: Counter({'b': 49.71152682983342})\n",
      "Value of fv_c: 3.485791930897275;\n",
      "Value of first derivative of fv_c: Counter({'c': 7.037686346377139})\n"
     ]
    }
   ],
   "source": [
    "# We can perform mathematical operations on our AutoDiffVector object\n",
    "fv_3 = fv_1 + 3\n",
    "\n",
    "fv_4 = sad.tan(fv_1) * 4\n",
    "\n",
    "# Addition\n",
    "print(\"\\nfv_3 Case:\")\n",
    "print(\"Value of fv_a: {};\\nValue of first derivative of fv_a: {}\".format(fv_3.objects['a'].val, fv_3.objects['a'].der))\n",
    "print(\"Value of fv_b: {};\\nValue of first derivative of fv_b: {}\".format(fv_3.objects['b'].val, fv_3.objects['b'].der))\n",
    "print(\"Value of fv_c: {};\\nValue of first derivative of fv_c: {}\".format(fv_3.objects['c'].val, fv_3.objects['c'].der))\n",
    "\n",
    "# Trigonometric operation and multiplication\n",
    "print(\"fv_4 Case:\")\n",
    "print(\"Value of fv_a: {};\\nValue of first derivative of fv_a: {}\".format(fv_4.objects['a'].val, fv_4.objects['a'].der))\n",
    "print(\"Value of fv_b: {};\\nValue of first derivative of fv_b: {}\".format(fv_4.objects['b'].val, fv_4.objects['b'].der))\n",
    "print(\"Value of fv_c: {};\\nValue of first derivative of fv_c: {}\".format(fv_4.objects['c'].val, fv_4.objects['c'].der))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian matrix generation\n",
    "Additionally, our package supports the generation of Jacobian matrices via our ```jacobian``` function. The function takes in an array of functions (defined through ```AutoDiff``` objects) and an array of variables that we differentiate our functions by. The function, then, prints out a ```NumPy``` array corresponding to the Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.06986342e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00, -6.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -2.40520843e+06,\n",
       "         0.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize autodiff objects\n",
    "g = sad.AutoDiff('g', 3.4)\n",
    "h = sad.AutoDiff('h', -4)\n",
    "i = sad.AutoDiff('i', 7)\n",
    "\n",
    "# Create functions\n",
    "f1 = sad.tan(g) + 4\n",
    "f2 = h**2 + 2*h - 12\n",
    "f3 = sad.log(i) - sad.exp(i * 2)\n",
    "\n",
    "# List of functions and variables\n",
    "variables = ['g', 'h', 'i', 'j']\n",
    "functions = [f1, f2, f3]\n",
    "\n",
    "# Obtain Jacobian\n",
    "sad.jacobian(variables, functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LELN7aJrhBSM"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DS9-gwUXpVe"
   },
   "source": [
    "# Software Organization\n",
    "\n",
    "## Directory structure\n",
    "\n",
    "\n",
    "\n",
    "        cs207-FinalProject/\n",
    "                    .coverage\n",
    "                    .coverage.xml\n",
    "                    .travis.yml\n",
    "                    LICENSE\n",
    "                    README.md\n",
    "                    requirements.txt\n",
    "                    setup.py\n",
    "                    setup.cfg\n",
    "                    build/\n",
    "                    docs/\n",
    "                          milestone_1.ipynb\n",
    "                          milestone_2.ipynb\n",
    "                          documentation.ipynb\n",
    "                          documentation.md\n",
    "                          fig/\n",
    "                              graph_1.png\n",
    "                              graph_2.png     \n",
    "                    dist/\n",
    "                    htmlcov/\n",
    "                    superautodiff/\n",
    "                          .coverage\n",
    "                          __init__.py\n",
    "                          autodiff.py\n",
    "                          autodiffreverse.py\n",
    "                          functions.py\n",
    "                    superautodiff.egg-info/      \n",
    "                    test-reports/\n",
    "                    tests/\n",
    "                          __init__.py\n",
    "                          .coverage\n",
    "                          .coverage.XML\n",
    "                          tests_autodiff.py\n",
    "                          tests_autodiffreverse.py       \n",
    "                          tests_autodiffvector.py         \n",
    "                \n",
    "<br>\n",
    "\n",
    "## Modules\n",
    "\n",
    "```superautodiff``` contains four modules in our corresponding to our package's four main competencies. The modules are summarily described here and explained in detail in the subsequent sections.\n",
    "- ```autodiff.py```: This module contains the core functionality of package—a forward mode AD library that is able to work with a vector of input variables for a vector of functions.\n",
    "- ```autodiffreverse.py```: This module contains the our reverse mode AD implementation.\n",
    "- ```functions.py```: This module contains the bulk of the mathematical operations used by our module along with our Jacobian function.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Docs\n",
    "\n",
    "The docs folder contains our documentation as a Python Notebook and as a Markdown file. We also have the notebooks for our first and second milestones.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Testing\n",
    "Testing is largely relevant to developers looking to edit and/or build upon our package; general users need not read this section. Our test suites - ``` test_autodiff.py ```, ``` test_autodiffreverse.py ```, and ``` test_autodiffvector.py ``` - are stored in our ```tests/``` folder and each script governs the testing of different aspects of our package. Our testing will be largely monitored through both Travis CI and CodeCov. Our GitHub repository will be fully integrated with Travis CI and CodeCov with relevant badges on our ```README.md``` to reflect the build status on Travis CI and the code coverage status on CodeCov. \n",
    "\n",
    "```superautodiff``` also supports ```pytest```. To run our tests, users will need to have ```pytest``` installed on their environment and navigate to the repository. Subsequently, users should run the following code:\n",
    "\n",
    "```python -m pytest```\n",
    "\n",
    "or:\n",
    "\n",
    "``` pytest ```\n",
    "\n",
    "This will run all our tests and provide summary statistics on the outcome of said tests.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Package Distribution\n",
    "Our package is distributed using PyPI. We use _setuptools_ and _wheel_ to generate our distribution archives and we use _twine_ to upload our package to PyPI.\n",
    "\n",
    "The reason for this choice of tools is that they are simple, easy-to-use, and reliable. Our package does not have many complicated dependencies; we, therefore, want to employ simple packaging and distribution tools to ensure that our package is easily distributed to users with minimal hassle.\n",
    "\n",
    "As mentioned above, users will simply have to call ```pip install superautodiff``` in order to install our package. The installation instructions and troubleshooting will be available on our GitHub repository.\n",
    "\n",
    "The dist, superautodiff.egg-info, and build folders are used for our package distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNRkcZgmsaWt"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3qiF7M_COe4"
   },
   "source": [
    "# Implementation \n",
    "\n",
    "Thus far, ```superautodiff``` has a working forward mode implementation and we have partly implemented multivariable automatic differentiation.\n",
    "\n",
    "## Data structures\n",
    "In our present implementation, the primary data structures used are Counters that we use to store our variable names, values, and derivative values in our ```AutoDiff``` class objects. The reason for this design choice is that we want to prevent cases where we have repeated variables when we implement multivariable automatic differentiation in subsequent milestones. We use ```Counter``` objects because they enable us to easily store our data in key-value pairs which makes it easier to evaluate the derivatives with respect to a particular variable.\n",
    "\n",
    "Our ```AutoDiffVector``` objects use the same underlying data structures but include an additional dictionary with the variable name as the key and ```AutoDiff``` objects as the values corresponding to each key. This helps to ensure that the ```AutoDiff``` objects are properly stored within the vectors and can be reliably called in order to return the specific ```AutoDiff``` objects.\n",
    "\n",
    "Our ```jacobian``` function uses ```NumPy``` arrays to generate Jacobian matrices largely because the arrays enable us to easily visualize and print out matrices; further, this enables our Jacobian matrix outputs to be usable in other matrix operations.\n",
    "\n",
    "## Dependencies\n",
    "Our package relies on the following external packages:\n",
    "- ```NumPy```: We use this to specify relevant mathematical operations within our package.\n",
    "\n",
    "- ```collections```: We use this to store our data in ```Counter``` objects.\n",
    "\n",
    "- ```math```: We use this for additional mathematical operations.\n",
    "\n",
    "## Dunder methods\n",
    "The following dunder methods have been overloaded in our implementation in order for our ```AutoDiff``` objects to be easily used in mathematical operations and the construction of mathematical functions:\n",
    "- ```__add__```: Modified to update the counter objects accordingly when addition is performed; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__radd__```: Modified to update the counter objects accordingly when addition is performed; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__sub__```: Modified to update the counter objects accordingly when subtraction is performed; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__rsub__```: Modified to update the counter objects accordingly when subtraction is performed; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__mul__```: Modified to update the counter objects accordingly when multiplication is performed; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__rmul__```: Modified to update the counter objects accordingly when multiplication is performed; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__neg__```: Modified such that all counter elements are made negative; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__truediv__```: Modified such that all counter elements are divided accordingly; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__rtruediv__```: Modified such that all counter elements are divided accordingly; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "- ```__pow__```: Modified such that the counter elements are appropriately exponentiated; modified to return ```AutoDiff``` objects.\n",
    "\n",
    "## Mathematical operations\n",
    "Our package implements the following mathematical operations using ```NumPy``` and ```math``` such that they can be used on ```AutoDiff``` objects with ease. All of the following functions can take in scalar values, vectors (Python lists), and ```AutoDiff``` objects. This is useful to users that seek to perform mathematical calculations and/or build up complicated mathematical functions using ```AutoDiff``` objects for derivative evaluation.\n",
    "\n",
    "### Trigonometric functions\n",
    "- ```sin(x)```\n",
    "\n",
    "- ```cos(x)```\n",
    "\n",
    "- ```tan(x)```\n",
    "\n",
    "- ```sec(x)```\n",
    "\n",
    "- ```csc(x)```\n",
    "\n",
    "- ```cot(x)```\n",
    "\n",
    "- ```arcsin(x)```\n",
    "\n",
    "- ```arccos(x)```\n",
    "\n",
    "- ```arctan(x)```\n",
    "\n",
    "- ```arcsec(x)```\n",
    "\n",
    "- ```arccsc(x)```\n",
    "\n",
    "- ```arccot(x)```\n",
    "\n",
    "- ```sinh(x)```\n",
    "\n",
    "- ```cosh(x)```\n",
    "\n",
    "- ```tanh(x)```\n",
    "\n",
    "\n",
    "### Logarithms and exponentials\n",
    "\n",
    "- ```log(x)``` of user specified base\n",
    "\n",
    "- ```exp(x)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12G47dsVtQVB"
   },
   "source": [
    "## Implementation illustrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and instantiation of objects\n",
    "Unlike the earlier section where we illustrate the usage of our package, here, we focus on the underlying methods used; the content is somewhat repetitive, but is retained for completeness. \n",
    "\n",
    "Once our module is imported, we can create ```AutoDiff``` objects that store the variable name and the value at which to evaluate the variables at. The object is mutable and can undergo mathematical operations in order to create complex mathematical functions; the object stores variable names, the values of the variables (given the value at which to evaluate the variables at), and the values of first derivatives of the variables (given the value at which to evaluate the variables at).\n",
    "\n",
    "\n",
    "Our package defines a class ```autodiff``` that takes a variable ```x``` as input. An ```autodiff``` object has two important attributes: \n",
    "- ```val``` - a scalar that contains the value of the function \n",
    "- ```der``` - a dictionary that stores the derivatives. For example:\n",
    "\n",
    " ```{\"a\":1, \"b\":1}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f1: 5.0;\n",
      "Value of first derivative of f1: Counter({'x_1': 1.0})\n"
     ]
    }
   ],
   "source": [
    "# Import module\n",
    "import superautodiff as sad\n",
    "\n",
    "# Initalize variable inputs and instantiate AutoDiff object\n",
    "value_to_evaluate = 5.0 \n",
    "variable_name = \"x_1\"\n",
    "f1 = sad.AutoDiff(variable_name, value_to_evaluate)\n",
    "\n",
    "# Illustrate how values and derivative values are stored\n",
    "print(\"Value of f1: {};\\nValue of first derivative of f1: {}\".format(f1.val, f1.der))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations using dunder methods\n",
    "The overloaded dunder methods enable the use of basic mathematical operations with ```AutoDiff``` objects. We do not check for the accuracy of our calculations here since that is already covered above in our Usage section; instead, we merely illustrate how the functions are used and the outputs they return in order to showcase our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f1_a: 10.0;\n",
      "Value of first derivative of f1_a: Counter({'x_1': 2.0})\n",
      "\n",
      "Value of f1_b: 10.0;\n",
      "Value of first derivative of f1_b: Counter({'x_1': 2.0})\n",
      "\n",
      "Value of f1_c: 15.0;\n",
      "Value of first derivative of f1_c: Counter({'x_1': 3.0})\n",
      "\n",
      "Value of f1_d: 25.0;\n",
      "Value of first derivative of f1_d: {'x_1': 10.0}\n",
      "\n",
      "Value of f1_e: 1.25;\n",
      "Value of first derivative of f1_e: {'x_1': 0.25}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition example\n",
    "f1_a = f1 + f1\n",
    "\n",
    "# Subtraction example\n",
    "f1_b = 3*f1 - f1\n",
    "\n",
    "# Multiplication example\n",
    "f1_c = f1 * 3\n",
    "\n",
    "# Exponent example\n",
    "f1_d = f1 ** 2\n",
    "\n",
    "# Division example\n",
    "f1_e = f1/4\n",
    "\n",
    "print(\"Value of f1_a: {};\\nValue of first derivative of f1_a: {}\\n\".format(f1_a.val, f1_a.der))\n",
    "print(\"Value of f1_b: {};\\nValue of first derivative of f1_b: {}\\n\".format(f1_b.val, f1_b.der))\n",
    "print(\"Value of f1_c: {};\\nValue of first derivative of f1_c: {}\\n\".format(f1_c.val, f1_c.der))\n",
    "print(\"Value of f1_d: {};\\nValue of first derivative of f1_d: {}\\n\".format(f1_d.val, f1_d.der))\n",
    "print(\"Value of f1_e: {};\\nValue of first derivative of f1_e: {}\\n\".format(f1_e.val, f1_e.der))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigonometric and logarithmic operations\n",
    "Similarly, our ```AutoDiff``` objects can be passed through our trigonometric and logarithmic functions. As before, we do not evaluate check the accuracy of the values as this has already been done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f1_f: -0.9589242746631385;\n",
      "Value of first derivative of f1_f: Counter({'x_1': 0.28366218546322625})\n",
      "\n",
      "Value of f1_g: -0.8390715290764524;\n",
      "Value of first derivative of f1_g: Counter({'x_1': 1.0880422217787395})\n",
      "\n",
      "Value of f1_h: -0.7470222972386602;\n",
      "Value of first derivative of f1_h: Counter({'x_1': 0.7790211562858627})\n",
      "\n",
      "Value of f1_i: 3269017.3724721107;\n",
      "Value of first derivative of f1_i: Counter({'x_1': 9807052.117416332})\n",
      "\n",
      "Value of f1_j: 2.302585092994046;\n",
      "Value of first derivative of f1_j: Counter({'x_1': 0.1})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sine example\n",
    "f1_f = sad.sin(f1)\n",
    "\n",
    "# Cosine example\n",
    "f1_g = sad.cos(f1*2)\n",
    "\n",
    "# Tangent example\n",
    "f1_h = sad.tan(f1/2)\n",
    "\n",
    "# Exp example\n",
    "f1_i = sad.exp(f1*3)\n",
    "\n",
    "# Natural logarithm example\n",
    "f1_j = sad.log(f1+5)\n",
    "\n",
    "print(\"Value of f1_f: {};\\nValue of first derivative of f1_f: {}\\n\".format(f1_f.val, f1_f.der))\n",
    "print(\"Value of f1_g: {};\\nValue of first derivative of f1_g: {}\\n\".format(f1_g.val, f1_g.der))\n",
    "print(\"Value of f1_h: {};\\nValue of first derivative of f1_h: {}\\n\".format(f1_h.val, f1_h.der))\n",
    "print(\"Value of f1_i: {};\\nValue of first derivative of f1_i: {}\\n\".format(f1_i.val, f1_i.der))\n",
    "print(\"Value of f1_j: {};\\nValue of first derivative of f1_j: {}\\n\".format(f1_j.val, f1_j.der))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector operations\n",
    "Vector operations are identical to those in the ```AutoDiff``` case. The ```AutoDiffVector``` objects essentially operate as dictionaries containing a set of ```AutoDiff``` objects that can undergo mathematical operations by relying on the ```AutoDiff``` methods that underlie the ```AutoDiffVector``` class attributes and methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Extension: Reverse Mode\n",
    "\n",
    "As an extension to our package, we have implemented reverse mode AD after having received approval for our extension. This section of our documentation details our reverse mode implementation and how it can be used.\n",
    "\n",
    "## Reverse mode\n",
    "The reverse mode is a method of performing AD that rests on an important mathematical property: that any differentiable algorithm can be translated into a sequecnce of assignments of basic mathematical operations.\n",
    "\n",
    "This, property, then motivates the first part of the reverse mode - namely, calculating the forward pass which essentially regenerates the function we would like to evaluate through its variable inputs. At this juncture, we store the value of the partial derivatives of each of the elementary functions.\n",
    "\n",
    "Subsequently, we compute all the derivatives in reverse order using the partial derivatives we have already obtained from our forward pass. This, then, enables us to evaluate the derivative of a function through the reverse mode.\n",
    "\n",
    "\n",
    "\n",
    "## ```AutoDiffReverse```\n",
    "We have create a new class of ```AutoDiff``` objects called ```AutoDiffReverse``` that operate similarly to regular ```AutoDiff``` objects except that these ```AutoDiffReverse``` objects rely on the reverse mode to evaluate derivatives rather than the forward mode.\n",
    "\n",
    "Much like the ```AutoDiff``` objects, our ```AutoDiffReverse``` objects have the following three attributes:\n",
    "- ```var```: name of variable\n",
    "- ```val```: value of the variable\n",
    "- ```der```: value of the derivative of the variable\n",
    "\n",
    "In the case of ```AutoDiffReverse```, the first input is the value at which to evaluate the derivative whilst the second input is the variable name (as opposed to how ```AutoDiff``` objects have it the other way around). This is because variable names are optional here since ```AutoDiffReverse``` objects are used in intermediary steps in our evaluative table.\n",
    "\n",
    "## Dependencies\n",
    "Our reverse mode implementation relies on ```NumPy``` for its mathematical functions used in ```AutoDiffReverse```'s mathematical operations.\n",
    "\n",
    "    Additionally ```AutoDiffReverse``` relies on ```Pandas``` as the evaluation table that is generated for the forward pass is created using a ```Pandas``` dataframe.\n",
    "\n",
    "## Data structures\n",
    "As mentioned, earlier, a key data structure used in our implementation is the ```Pandas``` dataframe that is used to store the evaluation table. The reason for this usage is because ```Pandas``` allows for the simple and quick generation of tables with which we can present our evaluation table cleanly. This improves user interpretation and makes it easy for users to extract specific values from our evaluation table.\n",
    "\n",
    "\n",
    "## Mathematical operations\n",
    "Not unlike our ```AutoDiff``` implementation, ```AutoDiffReverse``` includes the following mathematical operations using ```NumPy``` and ```math```. All of the following functions can take in scalar values, vectors (Python lists), and ```AutoDiffReverse``` objects. \n",
    "\n",
    "### Trigonometric functions\n",
    "- ```sin(x)```\n",
    "\n",
    "- ```cos(x)```\n",
    "\n",
    "- ```tan(x)```\n",
    "\n",
    "- ```sec(x)```\n",
    "\n",
    "- ```csc(x)```\n",
    "\n",
    "- ```cot(x)```\n",
    "\n",
    "- ```arcsin(x)```\n",
    "\n",
    "- ```arccos(x)```\n",
    "\n",
    "- ```arctan(x)```\n",
    "\n",
    "- ```arcsec(x)```\n",
    "\n",
    "- ```arccsc(x)```\n",
    "\n",
    "- ```arccot(x)```\n",
    "\n",
    "- ```sinh(x)```\n",
    "\n",
    "- ```cosh(x)```\n",
    "\n",
    "- ```tanh(x)```\n",
    "\n",
    "\n",
    "### Logarithms and exponentials\n",
    "\n",
    "- ```log(x)``` of user specified base\n",
    "\n",
    "- ```exp(x)```\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "We will now illustrate a typical use case of our reverse mode implementation in order to evaluate the derivatives of a given function at a given point and to generate the corresponding evaluation table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superautodiff import AutoDiffReverse\n",
    "from superautodiff.autodiffreverse import *\n",
    "\n",
    "# Instantiate AutoDiffReverse objects\n",
    "x1 = sad.AutoDiffReverse(4,\"x1\")\n",
    "x2 = sad.AutoDiffReverse(7,\"x2\")\n",
    "x3 = sad.AutoDiffReverse(3,\"x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f:  4.0\n"
     ]
    }
   ],
   "source": [
    "# We create a function using the AutoDiffReverse objects\n",
    "f = x1 - 3*x2 + x3*x2\n",
    "\n",
    "# Already we can obtain the value of f\n",
    "print(\"Value of f: \", f.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node</th>\n",
       "      <th>d1</th>\n",
       "      <th>d1value</th>\n",
       "      <th>d2</th>\n",
       "      <th>d2value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x1</td>\n",
       "      <td>x1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x2</td>\n",
       "      <td>x2</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x3</td>\n",
       "      <td>x3</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y1</td>\n",
       "      <td>x2</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y2</td>\n",
       "      <td>x1</td>\n",
       "      <td>1</td>\n",
       "      <td>y1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y3</td>\n",
       "      <td>x3</td>\n",
       "      <td>7</td>\n",
       "      <td>x2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y4</td>\n",
       "      <td>y2</td>\n",
       "      <td>1</td>\n",
       "      <td>y3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Node  d1 d1value  d2 d2value\n",
       "0   x1  x1       1   -       -\n",
       "0   x2  x2       1   -       -\n",
       "0   x3  x3       1   -       -\n",
       "0   y1  x2       3   -       -\n",
       "0   y2  x1       1  y1      -1\n",
       "0   y3  x3       7  x2       3\n",
       "0   y4  y2       1  y3       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assign and display the evaluation table for function f and examine\n",
    "forward_table = f.pass_table()\n",
    "display(forward_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of derivatives:  {'x1': 1, 'x2': 0.0, 'x3': 7.0}\n"
     ]
    }
   ],
   "source": [
    "# Input the forward table and array of variable names into reversepass to obtain derivatives\n",
    "der = reversepass(forward_table,['x1', 'x2', 'x3'])\n",
    "print(\"Value of derivatives: \",der)\n",
    "\n",
    "# Clear the table\n",
    "f.clear_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation table\n",
    "As illustrated above, the evaluation table is stored in ```forward_pass``` and can be accessed at any juncture in the reverse mode automatic differentiation process.\n",
    "\n",
    "The evaluation table consists of five columns; the first column (Node) contains the elements that are used to calculate the composite variables that are stored in the second (d1) and fourth columns (d2). Meanwhile, the third (d1value) and fifth (d2value) columns contain the derivative of each node with respect to d1 and d2.\n",
    "\n",
    "The table, therefore, contains the value of composite and intermediary variables that are used in the reverse mode AD process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work and Possible Extensions\n",
    "\n",
    "Although ```superautodiff``` is to be considered as a finished product that users should be able to satisfactorily install and use, our team has several ideas for possible extensions and developments that we hope to implement in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further vectorization\n",
    "Presently, although our ```AutoDiff``` objects are vectorizable as ```AutoDiffVector``` objects, the functionality of these ```AutoDiffVector``` objects is still very much limited. Our ```AutoDiffVector``` objects are able to work with scalars numerics and single ```AutoDiff``` objects but not vectors of these objects yet.\n",
    "\n",
    "We hope to implement further vectorization in order to allow for more flexible and complex vector operations for our ```AutoDiffVector``` objects. This implementation will involve working with ```NumPy``` arrays and potentially ```Numba``` if it is the case that our vector functions are found to be slow.\n",
    "\n",
    "We foresee that this will involve an overhaul of the ```AutoDiffVector```  class with more overrides so as to ensure that the vectors are fully functional and that we have some means of tracking the variables being passed in (which will be valuable as the vectorized operations might get quite messy when large numbers of variables are involved). \n",
    "\n",
    "Additionally, we expect that we will have to write several functions that help to simplify matrix operations; some functions are detailed as follows:\n",
    "- ```sad.dot(v_1, v_2)```: Takes in two ```AutoDiffVector``` objects and returns the dot product of the two.\n",
    "- ```sad.cross(v_1, v_2)```: Takes in two ```AutoDiffVector``` objects and returns the cross product of the two.\n",
    "- ```sad.determinant(v_1)```: Takes in a square ```AutoDiffVector``` object and returns the matrix determinant.\n",
    "- ```sad.eye(n)```: Takes in a scalar integer ```n``` and generates an $n \\times n$ identity matrix that can operate with ```AutoDiffVector``` objects\n",
    "- ```sad.trace(v_1)```: Takes in a square ```AutoDiffVector``` object and returns the trace of the matrix.\n",
    "- ```sad.reshape(v_1, dim)```: This will either be implemented as a function or an attribute; if it were implemented as a function, the function would take in a matrix and a tuple containing the matrix dimensions and reshape the given matrix to fit the specified dimensions similarly to how ```NumPy```'s reshape operates.\n",
    "\n",
    "This implementation will require quite a bit of work but will be very useful as it will enable users to perform a much broader set of operations with greater convenience. Additionally, if we were to revamp our vectorization implementation, we can also get our vectors to perform these operations far more quickly than in our current implementation which still relies heavily on inefficient loops. Presently, the speed at which our vectorized objects operate is somewhat of a shortcoming of our existing implementation due to its reliance on loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support for higher-order derivatives \n",
    "Another possible extension we hope to implement in the future is to build up our package's functionality in order to support operations involving higher-order derivatives.\n",
    "\n",
    "In theory, this approach would not be very difficult because both the forward mode and the reverse mode that we have implemented will have the core competency required to evaluate derivatives at higher-orders as well as cross-partial derivatives. We essentially need to extend our existing capabilities to differentiate variables multiple times and support cross-differentiation. The difficulty, we foresee, will most likely come from how we will need to overhaul our base classes in order to support this and how we can ensure that this additional functionality is user-friendly and intuitive; i.e. that we don't want our package to become cluttered, messy, and hard to use.\n",
    "\n",
    "Provisionally, we think that this will involve taking in the order of derivatives required as an input and developing a method of categorizing our derivatives and tracking all the derivatives and their orders. We will also need to refine our Counter output and variable naming convention to ensure that the outputs remain clear and informative for users.\n",
    "\n",
    "We think that expanding our functionality for higher-order derivatives is an important future feature that we definitely would like to have as we believe that it is important and useful for scientific computing. We can subsequently extend this slightly further and include functionality to allow for the generation of Hessian and border Hessian matrices which are extremely useful for optimization proclems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual interface\n",
    "\n",
    "Presenly, our reverse mode implementation implicitly constructs an evaluation table of values and a computational graph as part of its back-end operations that users do not really see (even though they can examine a simple evaluation table that we draw with ```Pandas```). \n",
    "\n",
    "Our team hopes that we can develop an extension that will enable our package to output more clear and informative evaluative tables that abide by design principles such that are informative and interpretable. The hope is that users will not only be able to use our package to evaluate derivatives, but also learn about the underlying processes (such as each step of the forward pass). Users should also be able to easily observe the composite derivative values in case they wish to obtain said values.\n",
    "\n",
    "Additionally, we hope to use tools such as ```Graphviz``` to generate the computational graph from our existing implmentation and/or use the d3 JavaScript library in order to create scalable vector graphics that can be used to visualize the computational graph. This can be particularly useful for users who are keen on learning and understanding the automatic differentiation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- Sondak, David. Lecture 10: Automatic Differentiation: The Forward Mode. Cambridge, MA; CS207 Fall '19\n",
    "- Hoffman, Philipp H.W. A Hitchhiker’s Guide to Automatic Differentiation. Numerical Algorithms, 72, 24 October 2015, 775-811, Springer Link, DOI 10.1007/s11075-015-0067-6.\n",
    "- Domke, Justin. A simple explanation of reverse-mode automatic differentiation. 24 March 2009. https://justindomke.wordpress.com/2009/03/24/a-simple-explanation-of-reverse-mode-automatic-differentiation/\n",
    "\n",
    "We would also like to thank our head instructor, Professor David Sondak, and our Teaching Fellow, Bhaven Patel, for their input, contribution, and support."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Milestone_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
