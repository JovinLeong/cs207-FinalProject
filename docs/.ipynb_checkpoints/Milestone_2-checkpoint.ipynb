{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oxk273fWvwK"
   },
   "source": [
    "# Milestone 1\n",
    "#### CS207 Final Project\n",
    "#### Group 1: _Team Gillet_\n",
    "#### Lucie Gillet, Sakari Jukarainen, Jovin Leong, Huahua Zheng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9-e0WQ0Xn1B"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I855YlHkXpGz"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "<br>\n",
    "\n",
    "Derivatives play a critical role in the natural and applied sciences, with optimization being one of the core applications involving derivatives. Traditionally, derivatives have been approached either symbolically or through numerical analysis (_e.g._ finite differences). Although numerical approaches to solving derivatives are simple to compute, they are prone to stability issues and round-off errors. Meanwhile, although symbolic derivatives enable the evaluation of derivatives to machine precision, the process is limited by its computational intensity. Recently, the size and complexity of the functions involving derivatives have grown; these demands necessitate an alternative to symbolic and numerical methods that is able to compute derivatives with higher accuracy at a lower cost. Automatic Differentiation (AD) addresses these issues by executing a sequence of elementary arithmetic operations to compute accurate derivatives. \n",
    "\n",
    "<br>\n",
    "\n",
    "Our team aims to develop a Python package, ```superautodiff```, that implements forward-mode AD. We also extend the package to solve simple optimization problems through gradient descent. This document will review some of the the mathematical foundations behind our approach and provide relevant information on documentation and usage of ```superautodiff```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tnOdVJ8XpNy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqgLFkC1XpJ8"
   },
   "source": [
    "# Background\n",
    "<br>\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "AD relies heavily on the chain rule and several other key mathematical concepts in order to compute derivatives. We now consider some background mathematical foundations that form the theoretical basis of our approach to AD.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Differential calculus**\n",
    "\n",
    "Differential calculus is concerned with the evaluation and study of gradients and/or rates of change. Numerically, we can formally define the derivative of a function $f$ evaluated at $a$ as:\n",
    "\n",
    "$$f'(a)=\\lim _{h\\to 0}{\\frac {f(a+h)-f(a)}{h}}$$.\n",
    "\n",
    "\n",
    "**Elementary functions and their derivatives**\n",
    "\n",
    "  Here are some examples of elementary functions used by AD and their corresponding derivatives:\n",
    "\n",
    "  <br>\n",
    "\n",
    "  **<center> Table 1. Elementary functions and their derivatives  </center>**\n",
    "  <br>\n",
    "\n",
    "| $$f(x)$$     | $$f'(x)$$    | \n",
    "| ------------- |:-------------:| \n",
    "| $$c$$        |         $0$   | \n",
    "| $$x$$        |         $1$   | \n",
    "| $$x^n$$      | $$nx^{n-1}$$ | \n",
    "| $$\\frac{1}{x}$$ | $$-\\frac{1}{x^2}$$ |\n",
    "| $$e^x$$      | $$e^x$$ | \n",
    "| $$log_ax$$      | $$\\frac{1}{x \\ln a}$$ | \n",
    "| $$\\ln x$$      | $$\\frac{1}{x}$$ | \n",
    "| $$\\sin(x)$$      | $$\\cos(x)$$ | \n",
    "| $$\\cos(x)$$      | $$-\\sin(x)$$ | \n",
    "| $$\\tan(x)$$      | $$\\frac{1}{\\cos^2x}$$ |<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Chain rule for composite functions**\n",
    "\n",
    "  The chain rule is a formula used to compute composite derivatives containing multiple variables. For instance, if we have a variable $z$ depending on $y$, which itself depends on $x$, we can subsequently employ the chain rule to express the derivative of $z$ with respect to $x$ is given by:\n",
    "\n",
    "<br>\n",
    "\n",
    "$${\\frac  {dz}{dx}}={\\frac  {dz}{dy}}\\cdot {\\frac  {dy}{dx}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**<center> The chain rule </center>**\n",
    "\n",
    "<br>\n",
    "\n",
    "**Forward and reverse mode**\n",
    "\n",
    "  For functions where we have intermediate components in our derivatives, we can keep track of the derivatives of each component using either of the following two modes: the forward mode and the reverse mode.\n",
    "  -        The forward mode starts with the input and computes the derivative with respect to the input using the chain rule at each subcomponent. The process involves storing the intermediate values of the derivatives of variables with respect to the input in order to evaluate the overall derivative: <br> <br> \n",
    "  $$\\frac{dw_i}{dx} = \\frac{dw_i}{dw_{i-1}}\\frac{dw_{i-1}}{dx}$$<br>\n",
    "   **<center> Forward mode </center>**  \n",
    "   \n",
    "<br>\n",
    "  \n",
    "  -        The reverse mode, meanwhile, involves both a forward pass that evaluates the values of the functions along with a backward pass that stores the derivatives of the output with respect to the different variables: <br> <br> $$\\frac{dy}{dw_i} = \\frac{dy}{dw_{i+1}}\\frac{dw_{i+1}}{dw_i}$$ <br>    **<center> Reverse mode </center>**\n",
    "\n",
    "<br>\n",
    "\n",
    "**Computational graph representation**\n",
    "\n",
    "  The elementary operations involved in the forward accumulation involved in the forward mode can be visually represented through a computational graph. For instance, the computational graph of the function $f(x)=x−exp\\{−2sin^2(4x)\\}^{[1]}$ is illustrated on Figure 1; Figure 2 presents a more complex computational graph.\n",
    "\n",
    "<br>\n",
    "\n",
    "  The graph breaks down the given function into a sequence of elementary operations that are visually charted out through the computational graph. The graph operates similarly to a flowchart and illustrates how each elementary operation modifies our initial parameter inputs in order to recover the function.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"fig/graph_1.png\" style=\"height:300px;\">\n",
    "\n",
    "**<center> Figure 1. A computational graph for $f(x)=x−exp\\{−2sin^2(4x)\\}^{[1]}$</center>**\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"fig/graph_2.png\" style=\"height:450px;\">\n",
    "\n",
    "**<center> Figure 2. A more complex computational graph</center>**\n",
    "\n",
    "<br>\n",
    "\n",
    "[1] D. Sondak, lecture 10, CS207 Fall '19\n",
    "\n",
    "<br>\n",
    "\n",
    "## What our package is doing\n",
    "Essentially, our package utilizes the aforementioned mathematical concepts in order to implement the AD through the forward mode. A primary function in our package, ```autodiff()```, takes in mathematical functions and corresponding points at which to evaluate the mathemetical functions and obtains an evaluative trace (similar to that of the graph structure above). Subsequently, this trace is used to perform differentiation on said mathematical function, using the chain rule to evaluate both the derivatives, the derivative values, and the current values at each component of the trace.\n",
    "\n",
    "Under the hood, we might perceive of the function's calculations as equivalent to populating the table illustrated in Table 2. This is basically the core of forward-mode AD; the functionality and operation of our package is discussed in greater detail in the subsequent section.\n",
    "\n",
    "<br>\n",
    "\n",
    "**<center>Table 2. An evaluation table for a foward-mode neural network</center>**\n",
    "\n",
    "| Trace | Elementary Function | Current Value | Elementary Function Derivative | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  | \n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------: | :-----------------: | \n",
    "| $x_{1}$ | $x$ | $x$ | $\\dot{ x}_{1}$ | $1$ | $0$ |\n",
    "| $x_{2}$ | $y$ | $y$ | $\\dot{x}_{2}$ | $0$ | $1$ |\n",
    "| $x_{3}$ | $w_{21}x_1$ | $w_{21}x$ | $w_{21}\\dot{x}_{1}$ | $w_{21}$ | $0$ |\n",
    "| $x_{4}$ | $w_{12}x_2$ | $w_{12}y$ | $w_{12}\\dot{x}_{2}$ | $0$ | $w_{12}$ |\n",
    "| $x_{5}$ | $w_{11}x_1$ | $w_{11}x$ | $w_{11}\\dot{x}_{1}$ | $w_{11}$ | $0$ |\n",
    "| $x_{6}$ | $w_{22}x_2$ | $w_{22}y$ | $w_{22}\\dot{x}_{2}$ | $0$ | $w_{22}$ |\n",
    "| $x_{7}$ | $x_4 + x_5$ | $w_{11}x + w_{12}y$ | $$\\dot{x}_{4} + \\dot{x}_{5}$$ | $w_{11}$ | $w_{12}$ |\n",
    "| $x_{8}$ | $x_3 + x_6$ | $w_{21}x + w_{22}y$ | $$\\dot{x}_{3} + \\dot{x}_{6}$$ | $w_{21}$ | $w_{22}$ |\n",
    "| $x_{9}$ | $z(x_7)$ | $z(w_{11}x + w_{12}y)$ | $$\\dot{x}_{7}z'(x_7)$$ | $w_{11}z'(w_{11}x + w_{12}y)$ | $w_{12}z'(w_{11}x + w_{12}y)$ |\n",
    "| $x_{10}$ | $z(x_8)$ | $z(w_{21}x + w_{22}y)$ | $$\\dot{x}_{8}z'(x_8)$$ | $w_{21}z'(w_{21}x + w_{22}y)$ | $w_{22}z'(w_{21}x + w_{22}y)$ |\n",
    "| $x_{11}$ | $w_{out,1}x_9$ | $$w_{out,1}z(w_{11}x + w_{12}y) $$  | $$w_{out,1}\\dot{x}_9$$ | $w_{out,1}w_{11}z'(w_{11}x + w_{12}y)$ | $w_{out,1}w_{12}z'(w_{11}x + w_{12}y)$ |\n",
    "| $x_{12}$ | $w_{out,2}x_{10}$ | $$w_{out,2}z(w_{21}x + w_{22}y) $$ | $$w_{out,2}\\dot{x}_{10}$$ | $w_{out,2}w_{21}z'(w_{21}x + w_{22}y)$ | $w_{out,2}w_{22}z'(w_{21}x + w_{22}y)$ |\n",
    "| $x_{13}$ | $x_{11} + x_{12}$ | $$w_{out,1}z(w_{11}x + w_{12}y) + w_{out,2}z(w_{21}x + w_{22}y) $$ | $$\\dot{x}_{11} + \\dot{x}_{12}$$ | $$w_{out,1}w_{11}z'(w_{11}x + w_{12}y) + w_{out,2}w_{21}z'(w_{21}x + w_{22}y)$$ | $$w_{out,1}w_{12}z'(w_{11}x + w_{12}y) + w_{out,2}w_{22}z'(w_{21}x + w_{22}y)$$ |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_r1OdOYlfrGh"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfGKPC2WrLmZ"
   },
   "source": [
    "# How to use ```superautodiff```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCWRWPWCrHHM"
   },
   "source": [
    "## User interaction with the package\n",
    "### Installation\n",
    "Our package will be distributed throughy PyPI (which will be detailed in the subsequent section). Users will first install the package by running:\n",
    "\n",
    "```pip install superautodiff -r requirements.txt```\n",
    "\n",
    "For more advanced users who think they have the required Python dependencies and do not wish to reinstall said dependencies, the following command should be run instead:\n",
    "\n",
    "```pip install superautodiff```\n",
    "\n",
    "Users will then need to import the package as in the above use case and will call ```autodiff()``` to instantiate AD objects. Users will then simply have to instantiate their functions and points in order to perform AD.\n",
    "\n",
    "### Importing\n",
    "After installing the package, users need to subsequently import it into their Python environment using the following import command:\n",
    "\n",
    "```python\n",
    "import superautodiff\n",
    "```\n",
    "\n",
    "Alternatively, it is recommended that users run the following import alias command for concision: \n",
    "```python\n",
    "import superautodiff as sad\n",
    "```\n",
    "\n",
    "Dependencies such as ```NumPy``` will be imported with the package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6AempE-gyjo"
   },
   "source": [
    "## Instantiating AD objects\n",
    "\n",
    "```superautodiff``` is a Python package and its core class is ```autodiff```. When initialized, ```autodiff()``` accepts an input $x \\in \\mathbb{R}$ (stored as the ```val``` attribute) and initializes the derivative (```der``` attribute) at $1$. The ```autodiff``` object then supports basic arithmetic operations (_e.g._ addition, multiplication) with integers, floats, and other ```autodiff``` objects. These operations will be implemented commutatively through dunder methods as appropriate. With an ```autodiff``` object, the user can evaluate the derivatives of a vector of functions at a specified vector of points.\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull first; TO DO: Use case\n",
    "\n",
    "## Example equation\n",
    "\n",
    "## Import\n",
    "\n",
    "## Do up the actual example and run the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5tooCHOlH1p"
   },
   "source": [
    "## Use case\n",
    "\n",
    "```python\n",
    "# Import the package\n",
    "import superautodiff as sad\n",
    "\n",
    "# Instantiate AutoDiff object with a scalar point\n",
    "a = 3.0\n",
    "f1 = sad.autodiff(a)\n",
    "\n",
    "print(f1.der[\"a\"])\n",
    "\n",
    ">>> 1\n",
    "```\n",
    "```python\n",
    "print(f1.val)\n",
    "\n",
    ">>> 3\n",
    "```\n",
    "```python\n",
    "# Define f2 as 4*f1\n",
    "f2 = 4*f1\n",
    "\n",
    "# Returns d/dx 4x\n",
    "print(f2.der[\"a\"])\n",
    "\n",
    ">>> 4\n",
    "```\n",
    "```python\n",
    "print(f2.val)\n",
    "\n",
    ">>>12\n",
    "```\n",
    "\n",
    "```python\n",
    "# Define f3 as (f2)^2\n",
    "f3 = f2**2\n",
    "\n",
    "# Returns d/dx (4x)^2 = 32x evaluated at x = a = 3:\n",
    "print(f3.der[\"a\"])\n",
    "\n",
    ">>> 96\n",
    "```\n",
    "```python\n",
    "# Returns (4x)^2 evaluated at x = a = 3\n",
    "print(f3.val)\n",
    "\n",
    ">>> 144\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LELN7aJrhBSM"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potentially remove multi and insert text into autodiff\n",
    "\n",
    "# Try out just using\n",
    "-pytest\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "-pytest --cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DS9-gwUXpVe"
   },
   "source": [
    "# Software Organization\n",
    "\n",
    "## Directory structure\n",
    "\n",
    "\n",
    "\n",
    "        cs207-FinalProject/\n",
    "                    LICENSE\n",
    "                    README.md\n",
    "                    requirements.txt\n",
    "                    setup.py\n",
    "                    setup.cfg\n",
    "                    travis.yml\n",
    "                    docs/\n",
    "                          milestone_1.md\n",
    "                          milestone_2.md\n",
    "                          milestone_1.ipynb\n",
    "                          milestone_2.ipynb\n",
    "                          fig/\n",
    "                              graph_1.png\n",
    "                              graph_2.png\n",
    "                    superautodiff/\n",
    "                          __init__.py\n",
    "                          autodiff.py\n",
    "                          functions.py\n",
    "                          optimize.py\n",
    "                          graddesc.py\n",
    "                    tests/\n",
    "                          __init__.py\n",
    "                          tests_autodiff.py      \n",
    "                    \n",
    "                \n",
    "<br>\n",
    "\n",
    "## Modules\n",
    "\n",
    "```superautodiff``` contains four modules corresponding to our package's four main competencies. The modules are summarily described here and explained in detail in the subsequent sections.\n",
    "- ```autodiff```: This module contains the core functionality of package—a forward mode AD library. \n",
    "- ```autodiffmulti```: Contains a user friendly interface for simultaneously evaluating partial derivatives with respect to a vector of input variables for a vector of functions.\n",
    "- ```optimize```: This module extends the base functionality of our forward mode AD library by providing functions to solve simple constrained and unconstrained optimization problems.\n",
    "- ```graddesc```: This module provides functions to perform gradient descent and stochastic gradient descent.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Testing\n",
    "Testing is largely relevant to developers looking to edit and/or build upon our package; general users need not read this section. Our test suite, ``` testsuite.py ```, is stored in our ```tests/``` folder; our testing will be largely monitored through both Travis CI and CodeCov. Our GitHub repository will be fully integrated with Travis CI and CodeCov with relevant badges on our ```README.md``` to reflect the build status on Travis CI and the code coverage status on CodeCov. \n",
    "\n",
    "```superautodiff``` also supports ```pytest```. To run our tests, users will need to have ```pytest``` installed on their environment and navigate to the repository. Subsequently, users should run the following code:\n",
    "\n",
    "```python -m pytest\n",
    "```\n",
    "\n",
    "This will run all our tests and provide summary statistics on the outcome of said tests.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Package Distribution\n",
    "Our package is distributed using PyPI. We use _setuptools_ and _wheel_ to generate our distribution archives and we use _twine_ to upload our package to PyPI.\n",
    "\n",
    "The reason for this choice of tools is that they are simple, easy-to-use, and reliable. Our package does not have many complicated dependencies; we, therefore, want to employ simple packaging and distribution tools to ensure that our package is easily distributed to users with minimal hassle.\n",
    "\n",
    "As mentioned above, users will simply have to call ```pip install superautodiff``` in order to install our package. The installation instructions and troubleshooting will be available on our GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNRkcZgmsaWt"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3qiF7M_COe4"
   },
   "source": [
    "# Implementation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-1yIvsQRgwM"
   },
   "source": [
    "The primary data structures used in ```superautodiff``` are vectors, lists, arrays, and dictionaries. Our package also replies on the external package ```NumPy```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48a5JxYgNwTd"
   },
   "source": [
    "Our package defines a class ```autodiff``` that takes a variable ```x``` as input. An ```autodiff``` object has two important attributes: \n",
    "- ```val``` - a scalar that contains the value of the function \n",
    "- ```der``` - a dictionary that stores the derivatives. For example:\n",
    "\n",
    " ```{\"a\":1, \"b\":1}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12G47dsVtQVB"
   },
   "source": [
    "```python\n",
    "class autodiff:\n",
    "# initiation\n",
    "  def __init__(self, x, val, der):\n",
    "    #  attributes \n",
    "    self.val \n",
    "    self.der \n",
    "\n",
    "# Override Dunder methods e.g.\n",
    "  def __add__(self, other):\n",
    "    # To be implemented\n",
    "  def __radd__(self, other):\n",
    "    # To be implemented\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I44BYr3ltbOa"
   },
   "source": [
    "```python\n",
    "# Univariate functions\n",
    "a = 3.0\n",
    "f1 = sad.autodiff(a)\n",
    "\n",
    "print(f1.der)\n",
    ">>> 1\n",
    "```\n",
    "```python\n",
    "print(f1.val)\n",
    ">>> 3\n",
    "```\n",
    "```python\n",
    "# Multivariate functions\n",
    "b = 4.0\n",
    "f2 = sad.autodiff(b)\n",
    "f3 = f1+f2\n",
    "\n",
    "print(f3.der[\"a\"])\n",
    "print(f3.val)\n",
    "\n",
    ">>> 1\n",
    ">>> 7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R74ijEcFOrCJ"
   },
   "source": [
    "Common operators such as `__add__`, `__radd__`,`__mul__`,`__rmul__`, `__mul__`,`__pow__`,`__rpow__` will be overloaded to process values and derivatives correctly and to return ```autodiff``` objects.\n",
    "\n",
    "We implement specific trigonometric functions such as ```sin(x)```, ```cos(x)```, ```tan(x)```, ```sec(x)```, ```csc(x)```, ```cot(x)```, ```arcsin(x)```, ```arccos(x)```, ```arctan(x)```, ```arcsec(x)```, ```arccsc(x)```, ```arccot(x)```.\n",
    "\n",
    "Additionally, we implement logarithmic functions such as ```log(x)``` and ```exp(x)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMNqP7b4OPZK"
   },
   "source": [
    "Elementary functions will be defined through ```NumPy``` such that said functions take in ```autodiff``` objects as inputs and returns ```autodiff``` objects with updated values and derivatives. \n",
    "Elementary functions will non-exhaustively include those we list in Table 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6sIE0X-uuL9"
   },
   "source": [
    "```python\n",
    "# An example\n",
    "def cos(x):\n",
    "    \"\"\"\n",
    "    Takes the cosine of original object and returns a new autodiff object\n",
    "    \"\"\"\n",
    "    # Need to init output \n",
    "    output.val = np.cos(x.val)\n",
    "    # For all i\n",
    "    output.der[i] = -np.sin(x)*x.der[i]     \n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RV_1UBTQQrp1"
   },
   "source": [
    "The module```autodiffmulti``` extends the base functionality of our package such that values and derivatives can be computed at the same time for multiple variables in multiple functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix this based on changes to autodiffmulti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "poVP4PDBvX6s"
   },
   "source": [
    "```python\n",
    "### Draft code \n",
    "class autodiffmulti():\n",
    "  def __init__(self, varibles):\n",
    "\n",
    "  def fit(self):\n",
    "\n",
    "  def derivative(self):\n",
    "    # Jacobian\n",
    "    nvar = len(p)\n",
    "    nfun = len(f)\n",
    "    mat = np.zeros((nfun, nvar))\n",
    "    mat[x,y] = f[x].eval(self).der[p[y]]\n",
    "    return mat\n",
    "\n",
    "  def value(self):\n",
    "    vec = [0]*nfun\n",
    "    vec[x] = f[x].eval(self).val\n",
    "    return vec\n",
    "\n",
    "# Define list of functions (here two functions)\n",
    "p = [3,2,1]\n",
    "f = [\"x1*x2\", \"x1 + x2 + x3\"]\n",
    "diff = autodiffmulti(p) \n",
    "diff.fit(f)\n",
    "\n",
    "print(diff.val)\n",
    "print(diff.der)\n",
    "\n",
    ">>> [6,6]\n",
    ">>> [[2,3,0],[1,1,1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Features\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Milestone_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
